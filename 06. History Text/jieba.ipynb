{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d388ee8d-b1c0-4423-936f-ed85143bdc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c8336c-539e-4112-8b8a-c5a0f14f4ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=pd.read_excel(r\"C:\\Users\\85054\\Desktop\\初中历史主观题作答数据集.xlsx\",sheet_name=0)\n",
    "data2=pd.read_excel(r\"C:\\Users\\85054\\Desktop\\初中历史主观题作答数据集.xlsx\",sheet_name=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "433a79e2-39bb-47d6-8d41-726e6184a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 静态配置\n",
    "stop_word_path = r\"C:\\Users\\85054\\Desktop\\cn_stopwords.txt\"\n",
    "corpus = data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2e06531-2dfd-4b00-967f-13a3c5dcf44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#临时删除文本元素\n",
    "def del_element(strings,symbles):\n",
    "    srcrep = {i:'' for i in symbles }\n",
    "    rep = dict((re.escape(k), v) for k, v in srcrep.items())\n",
    "    pattern = re.compile(\"|\".join(rep.keys()))\n",
    "    return pattern.sub(lambda m: rep[re.escape(m.group(0))], strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a0d65a7-946d-4998-9605-914989b6bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载停用词\n",
    "stop_words = stop_words = open(stop_word_path,'r',encoding='utf-8').read().split('\\n')+['\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5befaf56-7396-4b94-b06a-6bc588c6c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#过滤停用词\n",
    "def filter_stop_word(paper,stop_words):\n",
    "    return np.array(list(filter(lambda x: x not in stop_words,jieba.cut(del_element(paper,'\\n')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e51325c9-9455-4920-922a-681db9a35460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#过滤停用词\n",
    "def filter_stop_word(paper,stop_words):\n",
    "    return np.array(list(filter(lambda x: x not in stop_words,jieba.cut(del_element(paper,'\\n')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e1ad6fb-25c1-4360-afec-7782d1058581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取本地新闻\n",
    "def read_txt(corpus):\n",
    "    return np.array([re.sub('\\n','',str(word)) for word in tqdm(pd.read_csv(corpus).text,desc='加载文章')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b93e35-1890-4be9-bd73-ed45499dc0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b93e39-c4fe-4e94-92ed-bc6cf51c391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#只要中文\n",
    "def just_chinese(strings):\n",
    "    regStr = \".*?([\\u4E00-\\u9FA5]+).*?\"\n",
    "    expr = ''.join(re.findall(regStr, strings))\n",
    "    if expr:\n",
    "        return expr\n",
    "    return '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30df24e-b44d-44ee-b59e-0f10c7c1dec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', '第二次国共合作', '建立抗日民族统一展现', '领导全民族抗战', '成为抗日胜利的保障']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r\"[\\u3002\\uff1b\\uff0c\\uff1a\\u201c\\u201d\\uff08\\uff09\\u3001\\uff1f\\u300a\\u300b]\", \" ；第二次国共合作，建立抗日民族统一展现，领导全民族抗战，成为抗日胜利的保障\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9d9e4a2-72ae-4b85-86b3-04950adaed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#分词\n",
    "def split_word(original,temp_del=stop_words):\n",
    "    result = []\n",
    "    for paper in tqdm(original,desc='分词文章'):\n",
    "        chinese = just_chinese(paper)\n",
    "        temp_split_words = filter_stop_word(chinese,stop_words)\n",
    "        result.append(temp_split_words)\n",
    "    return np.array(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d696c3b1-1775-449c-9a73-39fc46e98ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 排序字典\n",
    "def sort_dict(dict_items):\n",
    "    sorted_tuple = np.array(sorted(dict_items.items(), key=lambda x: x[0], reverse=True))\n",
    "    return dict(zip(sorted_tuple[:,0],sorted_tuple[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "954a36e8-48a0-49c7-b6d9-b89586a641e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''数据预处理函数'''\n",
    "def data_preprocessing(corpus):\n",
    "    # 读取原文\n",
    "    read_original = read_txt(corpus) \n",
    "    # 倒入文章并分词\n",
    "    init_paper = split_word(read_original,stop_words)\n",
    "    # 所有单词降维到一维\n",
    "    all_words = np.array([j for i in tqdm(init_paper,desc='词列表降维') for j in i])\n",
    "    # 单词去重\n",
    "    word_vector = np.unique(all_words)\n",
    "    # 测量共有词汇量\n",
    "    m = all_words.size\n",
    "    init_word_dict = {word:(all_words==word).dot(np.ones(m))/m for word in tqdm(word_vector,desc='构建频率词典')}\n",
    "    #构建排序字典和特征向量 \n",
    "    word_dict = sort_dict(init_word_dict)\n",
    "    word_vector = np.array(list(word_dict)) \n",
    "    return word_dict,word_vector,read_original,init_paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd98b3-d3e7-4613-806b-bb865f1614a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "288b2119-32e4-4ebb-8ea8-76250110c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TF-IDF标准词袋'''\n",
    "def TF(paper_words,word_vector):\n",
    "    m = word_vector.size\n",
    "    init_TF = np.zeros(m)\n",
    "    for word in paper_words:\n",
    "        if word in word_vector:\n",
    "            index_ = np.argwhere(word_vector==word)[0][0]\n",
    "            init_TF[index_] += 1\n",
    "    return init_TF\n",
    "\n",
    "def IDF(paper_words_list,word_vector):\n",
    "    m = word_vector.size\n",
    "    init_IDF = np.zeros(m)\n",
    "    N = paper_words_list.shape\n",
    "    n = -1\n",
    "    for word in tqdm(word_vector,desc = 'IDF词汇'):\n",
    "        n += 1\n",
    "        for paper_arr in paper_words_list:\n",
    "            if word in paper_arr:\n",
    "                init_IDF[n] += 1\n",
    "    return np.log(N/(init_IDF+1))\n",
    "\n",
    "def TFIDF(paper_words_list,word_vector):\n",
    "    IDF_arr = IDF(init_paper,word_vector)\n",
    "    TF_arr = np.array([TF(paper,word_vector) for paper in tqdm(paper_words_list,desc = 'TF矩阵')])\n",
    "    return TF_arr*IDF_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff88dba0-b423-47e5-aa24-dfed4f5998f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7621b06-c91b-461f-87b8-7736f497ed79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
